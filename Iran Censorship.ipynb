{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from db_utils import query_hive_ssh, execute_hive_expression, get_hive_timespan\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "from labeling_utils import *\n",
    "from span_comparison import *\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import copy\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Hive table of Iran time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_dict = {'Iran': ['en', 'fa',]}\n",
    "start = '2015-05-11'\n",
    "stop = '2015-08-01'\n",
    "ts_table_name = 'iran_daily_ts'\n",
    "#create_hive_daily_ts(cp_dict, start, stop, 'iran_daily_ts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull down local version of Iran series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local_iran_daily_ts = get_all_features(cp_dict, cmp, ts_table_name, limit = 10000000, min_views = 100)\n",
    "#local_iran_daily_ts.to_csv('data/iran/iran_ts.tsv', sep = '\\t')\n",
    "local_iran_daily_ts = pd.read_csv('data/iran/iran_ts.tsv', sep = '\\t')\n",
    "\n",
    "print(local_iran_daily_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_iran_daily_ts.columns = [c.split('.')[1] if len(c.split('.')) == 2 else c for c in local_iran_daily_ts.columns]\n",
    "local_iran_daily_ts.rename(columns={'p': 'project', 'c': 'country', 't': 'page_title'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Timeseries From  Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df = pd.read_csv('./data/iran/gold_label_blocked_articles.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report_df.shape)\n",
    "report_df = report_df.merge(local_iran_daily_ts, on = ['project', 'country', 'page_title'])\n",
    "print(report_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in report_df.iterrows():\n",
    "    print(r['project'], r['en_page_title_x'])\n",
    "    plt.figure()\n",
    "    ts, ts_prop, f = plot_series(local_iran_daily_ts, start, stop, r, smooth = 3)\n",
    "    plt.show(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an image of examples for labelers to use as a reference point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Hive table with comparison of access rates from before and after the transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1start = '2015-05-01'\n",
    "s1stop = '2015-06-12'\n",
    "s2start = '2015-06-12'\n",
    "s2stop = '2015-07-24'\n",
    "\n",
    "cmp = PVSpanComparison([s1start, s1stop], [s2start, s2stop], 'censorship', dry = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select those time series with the greatest change in view counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_df = query_span_comparison(cmp, 'Iran', min_post_article_view = 100, min_wikidata_item_view = 500 )\n",
    "span_df.rename(columns={'p': 'project', 'c': 'country', 't': 'page_title'}, inplace=True)\n",
    "print(span_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_df.sort('normalized_wdc_view_proportion_delta', inplace  = True, ascending = 0)\n",
    "span_df['en_page_title'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_df.sort('normalized_tpc_view_proportion_delta', inplace  = True, ascending = 0)\n",
    "span_df['en_page_title'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting by normalized_tpc_view_proportion_delta seems better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create labeled set of Iran articles\n",
    "\n",
    "1. Sample Successive working sets of size 2k\n",
    "2. pick k random series + next top k extreme series and shuffle them\n",
    "3. labeler asked to score exaples based on example images (slider [-1, 1])\n",
    "3. labeler does not see titles \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_filename = './data/iran/iran_labels_2.txt'\n",
    "data_filename = './data/iran/iran_data_2.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_set_size = 100\n",
    "\n",
    "labels_file = open(labels_filename, \"a\")\n",
    "\n",
    "with open(labels_filename, \"r\") as f:\n",
    "    labeled_set = set(line.strip() for line in f)\n",
    "    \n",
    "data_file = open(data_filename, \"a\")\n",
    "\n",
    "end = False\n",
    "while not end:\n",
    "    \n",
    "    # get next working set:\n",
    "    working_set = []\n",
    "    i = 0\n",
    "    \n",
    "    for e in all_id_dicts:\n",
    "        if e['id'] not in labeled_set:\n",
    "            working_set.append(e)\n",
    "            i += 1      \n",
    "        if i == working_set_size:\n",
    "            break\n",
    "        \n",
    "    \n",
    "    df_ts = get_local_ts(working_set, en_titles = True)\n",
    "\n",
    "    for id_dict in working_set:\n",
    "        ts, ts_prop, f = plot_series(df_ts, start, stop, id_dict, smooth = 3)\n",
    "        print(id_dict['en_page_title'])\n",
    "        plt.show(f)\n",
    "        label = input()            \n",
    "        plt.close(f)\n",
    "        if label == 'x':\n",
    "            end = True\n",
    "            break\n",
    "        labels_file.write(id_dict['id'] + '\\n')\n",
    "        write_ts_to_file(id_dict, ts, ts_prop, label, data_file)\n",
    "        labeled_set.add(id_dict['id'])\n",
    "        \n",
    "labels_file.close()\n",
    "data_file.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train a model to predict spike\n",
    "For now, lets just focus on the proportion time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_utils import batch_iter\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(data_filename, sep = '\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(data_filename, sep = '\\t', header = None)\n",
    "data_df.fillna(0, inplace = True)\n",
    "y_df = pd.DataFrame([data_df[183] == 'y', data_df[183] != 'y']).transpose()\n",
    "y_df = y_df.astype(int)\n",
    "X_df = data_df.drop(183, axis=1)\n",
    "X_df = X_df.ix[:, 83:165]\n",
    "n = X_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "m = 500\n",
    "n = 10\n",
    "mu_1 = np.random.multivariate_normal([3], [[1]], n).squeeze()\n",
    "mu_2 = np.random.multivariate_normal([3], [[1]], n).squeeze()\n",
    "cov = np.identity(n) * 10\n",
    "\n",
    "X_1 = np.random.multivariate_normal(mu_1, cov, m)\n",
    "X_2 = np.random.multivariate_normal(mu_2, cov, m)\n",
    "\n",
    "y_1 = np.ones(m) \n",
    "y_2 = np.zeros(m)\n",
    "\n",
    "X = np.concatenate([X_1, X_2])\n",
    "y = np.concatenate([y_1, y_2])\n",
    "\n",
    "X_df = pd.DataFrame(X)\n",
    "\n",
    "y_df = pd.DataFrame([y == 1, y == 0]).transpose()\n",
    "y_df = y_df.astype(int)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 250\n",
    "batch_size = 200\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = n # 1st layer num features\n",
    "n_hidden_2 = n # 2nd layer num features\n",
    "n_hidden_3 = n # 2nd layer num features\n",
    "n_input = n # MNIST data input (img shape: 28*28)\n",
    "n_classes = 2 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph inputhttps://phabricator.wikimedia.org/T123292\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(_X, _weights, _biases):\n",
    "    layer_1 = tf.tanh(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1'])) \n",
    "    layer_2 = tf.tanh(tf.add(tf.matmul(layer_1, _weights['h2']), _biases['b2'])) \n",
    "    layer_3 = tf.tanh(tf.add(tf.matmul(layer_2, _weights['h3']), _biases['b3']))\n",
    "    return tf.matmul(layer_3, _weights['out']) + _biases['out']\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_3, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "# Training cycle\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    m = 0\n",
    "    batches = batch_iter(X_train, y_train, batch_size)\n",
    "    # Loop over all batches\n",
    "    for batch_xs, batch_ys in batches:\n",
    "        batch_m = len(batch_ys)\n",
    "        m += batch_m\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        # Compute average loss\n",
    "        avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys}) * batch_m\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost/m))\n",
    "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "        \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        train_acc = accuracy.eval({x: X_train, y: y_train}, session=sess)\n",
    "        test_acc = accuracy.eval({x: X_test, y: y_test}, session=sess)\n",
    "        print (\"Accuracy:\",train_acc )\n",
    "        print (\"Accuracy:\",test_acc ) \n",
    "        train_accs.append(train_acc)\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "\n",
    "print (\"Optimization Finished!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_ts_raw = get_all_series({'Iran':['en',]}, limit = 10000, min_views = 100)\n",
    "df_all_ts = pd.DataFrame([parse(ts) for ts in df_all_ts_raw['ts']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = tf.nn.softmax(pred)\n",
    "results = probs.eval(feed_dict={x: df_all_ts.values}, session=sess).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_ts_raw['probs'] = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_ts_raw.sort('probs', ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_ts_raw.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, line in enumerate(open(data_filename, 'r')):\n",
    "    if (len(line.split('\\t'))) == 183:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get daily per country and per article pageview time series for a set of country project pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_dict = {'Iran':                        ['en', 'fa',],\n",
    "           'Saudi Arabia':                ['en', 'ar',],\n",
    "           'Turkey':                      ['en', 'tr',],\n",
    "           'Rebublic of Korea':           ['en', 'ko',],\n",
    "           'Iraq':                        ['en', 'ar',],\n",
    "           'Cuba':                        ['en', 'es',],\n",
    "           'Venezuela':                   ['en', 'es',],\n",
    "           'Pakistan':                    ['en', 'ur',],\n",
    "           'Vietnam':                     ['en', 'vi',],\n",
    "           'Singapore':                   ['en', 'zh',],\n",
    "           'Uzbekistan':                  ['en', 'uz',],\n",
    "           'Nigeria':                     ['en', 'en',],\n",
    "           'Egypt':                       ['en', 'ar',],\n",
    "           'Thailand':                    ['en', 'th',],\n",
    "           'Morocco':                     ['en', ],\n",
    "           'Bangladesh':                  ['en', ],\n",
    "           'United States':               ['en', ],\n",
    "           'China':                       ['en', ],\n",
    "           'Russia':                      ['en', 'ru',],\n",
    "          }\n",
    "\n",
    "start = '2015-05-11'\n",
    "stop = '2015-08-01'\n",
    "#create_hive_daily_ts(cp_dict, start, stop, 'daily_ts2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts.index  = pd.to_datetime(df_ts.day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts['proportion'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [ 'Lesbian', 'LGBT', 'Gay', 'Transgender', 'Bisexuality', 'Homosexuality']\n",
    "fig_dir = './figs_queer'\n",
    "df = get_local_ts(cp_dict, articles) \n",
    "plot_all_series(df, start, stop, cp_dict, articles, fig_dir, smooth = 7 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = ['Sex', 'Anal_sex', 'BDSM', 'Brazzers', 'Cunnilingus', 'Dildo', 'Fellatio', 'Oral_sex', 'Human_penis', 'Vulva', 'Scrotum', 'Vagina']\n",
    "fig_dir = './figs_sex'\n",
    "df = get_local_ts(cp_dict, articles) \n",
    "plot_all_series(df, start, stop, cp_dict, articles, fig_dir, smooth = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = ['Mustafa_Kemal_Atatürk', 'Human_penis', 'Vulva', 'Scrotum', 'Vagina', 'Opinion_polling_for_the_Turkish_general_election,_June_2015']\n",
    "fig_dir = './figs_turkey_suggestions'\n",
    "cp = {'Turkey': ['tr', 'en']}\n",
    "df = get_local_ts(cp, articles) \n",
    "plot_all_series(df, start, stop, cp, articles, fig_dir, smooth = 7 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = ['Salman_of_Saudi_Arabia']\n",
    "fig_dir = './figs_saudi_king'\n",
    "cp = {'Saudi Arabia': ['ar', 'en']}\n",
    "df = get_local_ts(cp, articles) \n",
    "plot_all_series(df, start, stop, cp, articles, fig_dir, smooth = 7 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_censorship = 'Iran'\n",
    "c_control = 'United States'\n",
    "countries = [c_censorship, c_control]\n",
    "projects = ['en.wikipedia', 'fa.wikipedia']\n",
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 'Iran'\n",
    "\n",
    "# en articles from paper\n",
    "#blocked_articles = list(pd.read_csv('./data/blocked_articles.tsv')['article']) \n",
    "\n",
    "d_censorship = pd.read_csv('./data/https_transition_comparison.tsv', sep = '\\t', encoding = 'utf8')\n",
    "\n",
    "# outliers in censored country\n",
    "\n",
    "outlier_articles = [tuple(x) for x in d_censorship[d_censorship['country'] == c][['project', 'title']][:n].values]\n",
    "\n",
    "# their english counter parts\n",
    "en_outlier_articles = [ ('en.wikipedia', x) for x  in d_censorship[d_censorship['country'] == c][:n]['en_title']]\n",
    "\n",
    "# get times series for all\n",
    "articles = set([str(e[1]) for e in outlier_articles + en_outlier_articles if \"'\" not in str(e[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_countries(start, stop, c_censorship, c_control, a_censorship, a_control, smooth = 4):\n",
    "    f, axarr = plt.subplots(2, sharex=True)\n",
    "    \n",
    "    # plot transition point\n",
    "    english_end = datetime.strptime('2015-06-12 09:40', \"%Y-%m-%d %H:%M\") # End transition of English Wikipedia, including Mobile\n",
    "    axarr[0].axvline(english_end, color='red', label = 'HTTPS transition', linewidth=0.5)\n",
    "    axarr[1].axvline(english_end, color='red', label = 'HTTPS transition', linewidth=0.5)\n",
    "\n",
    "    # plot ts for article in censored country\n",
    "    project = a_censorship[0]\n",
    "    title = str(a_censorship[1])\n",
    "    ts0 = get_series(start, stop, project, c_censorship, title)\n",
    "    ts0 = pd.rolling_mean(ts0, smooth)\n",
    "    axarr[0].plot(ts0.index, ts0.values)\n",
    "    ylabel = c_censorship \n",
    "    axarr[0].set_ylabel(ylabel)\n",
    "    \n",
    "        \n",
    "    # plot ts for articles in control\n",
    "    en_project = a_control[0]\n",
    "    en_title = str(a_control[1])\n",
    "    ts1 = get_series(start, stop, en_project, c_control, en_title)\n",
    "    ts1 = pd.rolling_mean(ts1, smooth)\n",
    "    axarr[1].plot(ts1.index, ts1.values)\n",
    "    ylabel = c_control #+ us_article[0].split('.')[0] + ' ' + us_article[1]\n",
    "    axarr[1].set_ylabel(ylabel)\n",
    "    \n",
    "    axarr[0].set_title(project.split('.')[0] + ' ' + en_title)\n",
    "    \n",
    "    fig_dir = './figs_' + c_censorship\n",
    "    if en_title is not np.nan:\n",
    "        fig_name =  en_title  +  '.pdf'\n",
    "    else:\n",
    "        fig_name =  title  +  '.pdf'\n",
    "    \n",
    "    fig_name = fig_name.replace('/', '-')\n",
    "    plt.savefig(os.path.join(fig_dir, fig_name))\n",
    "    plt.close(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = './figs_' + c_censorship\n",
    "if os.path.exists(fig_dir):\n",
    "    shutil.rmtree(fig_dir)\n",
    "os.makedirs(fig_dir)\n",
    "    \n",
    "for i, article in enumerate(outlier_articles):\n",
    "    compare_countries(start, stop, c_censorship, c_control, article, en_outlier_articles[i] , smooth = 24)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
